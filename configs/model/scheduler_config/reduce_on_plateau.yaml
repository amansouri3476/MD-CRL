scheduler:
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    mode: 'min'
    factor: 0.5
    patience: 5
    threshold: 0.01
    threshold_mode: 'abs'
    cooldown: 5
    min_lr: 1e-7
    eps: 1e-8
    verbose: True
    
scheduler_dict:
#     scheduler:  # scheduler instance, will be passed inside configure_optimizer
    interval: "step"  # The unit of the scheduler's step size. 'step' or 'epoch
    frequency: 50  # corresponds to updating the learning rate after every `frequency` epoch/step
    monitor: val_r2_hz_z # train_loss # Used by a LearningRateMonitor callback when ReduceLROnPlateau is used
    name: "ReduceLROnPlateau"