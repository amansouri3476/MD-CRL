_target_: models.mixing_autoencoder_pl.MixingAutoencoderPL

defaults:
  - scheduler_config: reduce_on_plateau # options: null, reduce_on_plateau, linear, polynomial
  - optimizer: adam # options: adamw, adam
  - autoencoder: linear_ae_synthetic # options:
  - additional_logger: null # reconstruction_logger


top_k: 5
num_domains: ${datamodule.dataset.num_domains}
mismatch_dims: 0
z_dim: ${add_int:${datamodule.dataset.z_dim},${mult_int:${model.mismatch_dims},2}}
z_dim_invariant: ${add_int:${datamodule.dataset.z_dim_invariant},${model.mismatch_dims}} # make sure it is smaller than encoder's latent_dim
# penalty_criterion: "minmax" # options: minmax, stddev
penalty_criterion:
  minmax: 1.
  stddev: 0.
  domain_classification: 0.

loss_transform: "mse"
penalty_weight: 0.01
stddev_threshold: 1.0
stddev_eps: 0.0001
hinge_loss_weight: 25.0
wait_steps: 0 # 500 # 2000
linear_steps: 1 # 2000 # 3000


# full ckpt containing state_dict, callbacks, optimizer, hyperparameters, etc.
pl_model_ckpt_path: null
# set this to null to train from scratch.  NOTE: This path (.pt) only contains the 
# state_dict (to be flexible), if the full ckpt is required (not flexible w/ 
# different # of slots, then load .ckpt file)
autoencoder_ckpt_path: ${retrieve_encoder_state_dict:${model.pl_model_ckpt_path}}
autoencoder_freeze: False


logging_name: "autoencoder_${datamodule.datamodule_name}_${datamodule.dataset.z_dim}"
