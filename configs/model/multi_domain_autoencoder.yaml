_target_: models.multi_domain_autoencoder_pl.MultiDomainAutoencoderPL

defaults:
  - scheduler_config: null # options: reduce_on_plateau, linear, polynomial
  - optimizer: adamw # options: adamw, adam
  - autoencoder: fc_ae # options:
  - additional_logger: null # reconstruction_logger


top_k: 5
num_domains: ${datamodule.dataset.num_domains}
z_dim: 16 # make sure it is smaller than encoder's latent_dim
penalty_weight: 0.01

# full ckpt containing state_dict, callbacks, optimizer, hyperparameters, etc.
pl_model_ckpt_path: null
# set this to null to train from scratch.  NOTE: This path (.pt) only contains the 
# state_dict (to be flexible), if the full ckpt is required (not flexible w/ 
# different # of slots, then load .ckpt file)
autoencoder_ckpt_path: ${retrieve_encoder_state_dict:${model.pl_model_ckpt_path}}
autoencoder_freeze: False


logging_name: "autoencoder_${datamodule.datamodule_name}_${model.autoencoder.latent_dim}"
